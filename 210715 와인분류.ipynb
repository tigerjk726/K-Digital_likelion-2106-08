{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82a0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (4547, 12)\n",
      "x_test.shape = (1950, 12)\n",
      "y_train.shape = (4547,)\n",
      "y_test.shape = (1950,)\n",
      "[0 0 0 0 1]\n",
      "Epoch 1/200\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.8995 - accuracy: 0.7450WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.3761 - accuracy: 0.8837\n",
      "Epoch 2/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.2848 - accuracy: 0.9276\n",
      "Epoch 3/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.2398 - accuracy: 0.9294\n",
      "Epoch 4/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.1851 - accuracy: 0.9303\n",
      "Epoch 5/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.1742 - accuracy: 0.9380\n",
      "Epoch 6/200\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.1567 - accuracy: 0.9400\n",
      "Epoch 7/200\n",
      "23/23 [==============================] - 0s 825us/step - loss: 0.1449 - accuracy: 0.9430\n",
      "Epoch 8/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9532\n",
      "Epoch 9/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9543\n",
      "Epoch 10/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9587\n",
      "Epoch 11/200\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.1190 - accuracy: 0.9624\n",
      "Epoch 12/200\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1099 - accuracy: 0.9637\n",
      "Epoch 13/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0983 - accuracy: 0.9686\n",
      "Epoch 14/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0939 - accuracy: 0.9705\n",
      "Epoch 15/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0924 - accuracy: 0.9712\n",
      "Epoch 16/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.1018 - accuracy: 0.9655\n",
      "Epoch 17/200\n",
      "23/23 [==============================] - 0s 608us/step - loss: 0.1092 - accuracy: 0.9657\n",
      "Epoch 18/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0988 - accuracy: 0.9681\n",
      "Epoch 19/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.1015 - accuracy: 0.9655\n",
      "Epoch 20/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0823 - accuracy: 0.9760\n",
      "Epoch 21/200\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.0854 - accuracy: 0.9725\n",
      "Epoch 22/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9740\n",
      "Epoch 23/200\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.0736 - accuracy: 0.9815\n",
      "Epoch 24/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9778\n",
      "Epoch 25/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9800\n",
      "Epoch 26/200\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.0785 - accuracy: 0.9749\n",
      "Epoch 27/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0733 - accuracy: 0.9789\n",
      "Epoch 28/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0776 - accuracy: 0.9762\n",
      "Epoch 29/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0967 - accuracy: 0.9723\n",
      "Epoch 30/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0868 - accuracy: 0.9751\n",
      "Epoch 31/200\n",
      "23/23 [==============================] - 0s 867us/step - loss: 0.0686 - accuracy: 0.9791\n",
      "Epoch 32/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0679 - accuracy: 0.9782\n",
      "Epoch 33/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0854 - accuracy: 0.9736\n",
      "Epoch 34/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0806 - accuracy: 0.9749\n",
      "Epoch 35/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0668 - accuracy: 0.9811\n",
      "Epoch 36/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0729 - accuracy: 0.9773\n",
      "Epoch 37/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0699 - accuracy: 0.9773\n",
      "Epoch 38/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0621 - accuracy: 0.9815\n",
      "Epoch 39/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0648 - accuracy: 0.9806\n",
      "Epoch 40/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0703 - accuracy: 0.9798\n",
      "Epoch 41/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0713 - accuracy: 0.9787\n",
      "Epoch 42/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0702 - accuracy: 0.9802\n",
      "Epoch 43/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0624 - accuracy: 0.9826\n",
      "Epoch 44/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0593 - accuracy: 0.9817\n",
      "Epoch 45/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0677 - accuracy: 0.9784\n",
      "Epoch 46/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0697 - accuracy: 0.9800\n",
      "Epoch 47/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0688 - accuracy: 0.9802\n",
      "Epoch 48/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0600 - accuracy: 0.9822\n",
      "Epoch 49/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0635 - accuracy: 0.9815\n",
      "Epoch 50/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0677 - accuracy: 0.9800\n",
      "Epoch 51/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0581 - accuracy: 0.9828\n",
      "Epoch 52/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0583 - accuracy: 0.9820\n",
      "Epoch 53/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0592 - accuracy: 0.9798\n",
      "Epoch 54/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0607 - accuracy: 0.9809\n",
      "Epoch 55/200\n",
      "23/23 [==============================] - 0s 477us/step - loss: 0.0603 - accuracy: 0.9815\n",
      "Epoch 56/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0593 - accuracy: 0.9824\n",
      "Epoch 57/200\n",
      "23/23 [==============================] - 0s 477us/step - loss: 0.0548 - accuracy: 0.9842\n",
      "Epoch 58/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0634 - accuracy: 0.9782\n",
      "Epoch 59/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0565 - accuracy: 0.9791\n",
      "Epoch 60/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0496 - accuracy: 0.9842\n",
      "Epoch 61/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0517 - accuracy: 0.9833\n",
      "Epoch 62/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0517 - accuracy: 0.9833\n",
      "Epoch 63/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0501 - accuracy: 0.9837\n",
      "Epoch 64/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0522 - accuracy: 0.9837\n",
      "Epoch 65/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0552 - accuracy: 0.9800\n",
      "Epoch 66/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0619 - accuracy: 0.9826\n",
      "Epoch 67/200\n",
      "23/23 [==============================] - 0s 651us/step - loss: 0.0640 - accuracy: 0.9809\n",
      "Epoch 68/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0578 - accuracy: 0.9824\n",
      "Epoch 69/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0578 - accuracy: 0.9822\n",
      "Epoch 70/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0696 - accuracy: 0.9767\n",
      "Epoch 71/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0681 - accuracy: 0.9809\n",
      "Epoch 72/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0735 - accuracy: 0.9758\n",
      "Epoch 73/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0526 - accuracy: 0.9831\n",
      "Epoch 74/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0646 - accuracy: 0.9773\n",
      "Epoch 75/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0488 - accuracy: 0.9844\n",
      "Epoch 76/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0514 - accuracy: 0.9839\n",
      "Epoch 77/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0718 - accuracy: 0.9771\n",
      "Epoch 78/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0502 - accuracy: 0.9826\n",
      "Epoch 79/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0504 - accuracy: 0.9850\n",
      "Epoch 80/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0506 - accuracy: 0.9837\n",
      "Epoch 81/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0484 - accuracy: 0.9842\n",
      "Epoch 82/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0732 - accuracy: 0.9767\n",
      "Epoch 83/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0619 - accuracy: 0.9800\n",
      "Epoch 84/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0586 - accuracy: 0.9791\n",
      "Epoch 85/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0527 - accuracy: 0.9813\n",
      "Epoch 86/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0509 - accuracy: 0.9826\n",
      "Epoch 87/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0479 - accuracy: 0.9844\n",
      "Epoch 88/200\n",
      "23/23 [==============================] - 0s 738us/step - loss: 0.0521 - accuracy: 0.9828\n",
      "Epoch 89/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0503 - accuracy: 0.9833\n",
      "Epoch 90/200\n",
      "23/23 [==============================] - 0s 608us/step - loss: 0.0467 - accuracy: 0.9850\n",
      "Epoch 91/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0522 - accuracy: 0.9837\n",
      "Epoch 92/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0520 - accuracy: 0.9831\n",
      "Epoch 93/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0687 - accuracy: 0.9782\n",
      "Epoch 94/200\n",
      "23/23 [==============================] - 0s 641us/step - loss: 0.0669 - accuracy: 0.9784\n",
      "Epoch 95/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0475 - accuracy: 0.9855\n",
      "Epoch 96/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0453 - accuracy: 0.9848\n",
      "Epoch 97/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0455 - accuracy: 0.9861\n",
      "Epoch 98/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0493 - accuracy: 0.9826\n",
      "Epoch 99/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0499 - accuracy: 0.9837\n",
      "Epoch 100/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0500 - accuracy: 0.9826\n",
      "Epoch 101/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0490 - accuracy: 0.9839\n",
      "Epoch 102/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0458 - accuracy: 0.9857\n",
      "Epoch 103/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0495 - accuracy: 0.9844\n",
      "Epoch 104/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0488 - accuracy: 0.9846\n",
      "Epoch 105/200\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0470 - accuracy: 0.9850\n",
      "Epoch 106/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0498 - accuracy: 0.9850\n",
      "Epoch 107/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0554 - accuracy: 0.9844\n",
      "Epoch 108/200\n",
      "23/23 [==============================] - 0s 477us/step - loss: 0.0481 - accuracy: 0.9848\n",
      "Epoch 109/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0576 - accuracy: 0.9826\n",
      "Epoch 110/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0495 - accuracy: 0.9835\n",
      "Epoch 111/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0490 - accuracy: 0.9844\n",
      "Epoch 112/200\n",
      "23/23 [==============================] - 0s 501us/step - loss: 0.0476 - accuracy: 0.9844\n",
      "Epoch 113/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0450 - accuracy: 0.9859\n",
      "Epoch 114/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0497 - accuracy: 0.9844\n",
      "Epoch 115/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0524 - accuracy: 0.9839\n",
      "Epoch 116/200\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.0465 - accuracy: 0.9846\n",
      "Epoch 117/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0528 - accuracy: 0.9809\n",
      "Epoch 118/200\n",
      "23/23 [==============================] - 0s 679us/step - loss: 0.0541 - accuracy: 0.9824\n",
      "Epoch 119/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0509 - accuracy: 0.9813\n",
      "Epoch 120/200\n",
      "23/23 [==============================] - 0s 737us/step - loss: 0.0486 - accuracy: 0.9822\n",
      "Epoch 121/200\n",
      "23/23 [==============================] - 0s 780us/step - loss: 0.0558 - accuracy: 0.9813\n",
      "Epoch 122/200\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.0566 - accuracy: 0.9809\n",
      "Epoch 123/200\n",
      "23/23 [==============================] - 0s 824us/step - loss: 0.0599 - accuracy: 0.9804\n",
      "Epoch 124/200\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0507 - accuracy: 0.9833\n",
      "Epoch 125/200\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.0670 - accuracy: 0.9767\n",
      "Epoch 126/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0578 - accuracy: 0.9811\n",
      "Epoch 127/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0543 - accuracy: 0.9828\n",
      "Epoch 128/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0493 - accuracy: 0.9837\n",
      "Epoch 129/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0445 - accuracy: 0.9846\n",
      "Epoch 130/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0529 - accuracy: 0.9824\n",
      "Epoch 131/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0466 - accuracy: 0.9846\n",
      "Epoch 132/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0446 - accuracy: 0.9864\n",
      "Epoch 133/200\n",
      "23/23 [==============================] - 0s 650us/step - loss: 0.0478 - accuracy: 0.9848\n",
      "Epoch 134/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0413 - accuracy: 0.9864\n",
      "Epoch 135/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0414 - accuracy: 0.9868\n",
      "Epoch 136/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0416 - accuracy: 0.9879\n",
      "Epoch 137/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0428 - accuracy: 0.9866\n",
      "Epoch 138/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0475 - accuracy: 0.9855\n",
      "Epoch 139/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0467 - accuracy: 0.9846\n",
      "Epoch 140/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0436 - accuracy: 0.9857\n",
      "Epoch 141/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0420 - accuracy: 0.9866\n",
      "Epoch 142/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0466 - accuracy: 0.9846\n",
      "Epoch 143/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0434 - accuracy: 0.9875\n",
      "Epoch 144/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0414 - accuracy: 0.9872\n",
      "Epoch 145/200\n",
      "23/23 [==============================] - 0s 694us/step - loss: 0.0421 - accuracy: 0.9857\n",
      "Epoch 146/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0425 - accuracy: 0.9866\n",
      "Epoch 147/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0454 - accuracy: 0.9853\n",
      "Epoch 148/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0439 - accuracy: 0.9853\n",
      "Epoch 149/200\n",
      "23/23 [==============================] - 0s 565us/step - loss: 0.0429 - accuracy: 0.9857\n",
      "Epoch 150/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0426 - accuracy: 0.9857\n",
      "Epoch 151/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0587 - accuracy: 0.9824\n",
      "Epoch 152/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0515 - accuracy: 0.9813\n",
      "Epoch 153/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0444 - accuracy: 0.9853\n",
      "Epoch 154/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0425 - accuracy: 0.9866\n",
      "Epoch 155/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0418 - accuracy: 0.9855\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 520us/step - loss: 0.0498 - accuracy: 0.9831\n",
      "Epoch 157/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0542 - accuracy: 0.9804\n",
      "Epoch 158/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0459 - accuracy: 0.9842\n",
      "Epoch 159/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0434 - accuracy: 0.9866\n",
      "Epoch 160/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0401 - accuracy: 0.9866\n",
      "Epoch 161/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0529 - accuracy: 0.9826\n",
      "Epoch 162/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0496 - accuracy: 0.9842\n",
      "Epoch 163/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0406 - accuracy: 0.9879\n",
      "Epoch 164/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0424 - accuracy: 0.9868\n",
      "Epoch 165/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0422 - accuracy: 0.9853\n",
      "Epoch 166/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0422 - accuracy: 0.9872\n",
      "Epoch 167/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0438 - accuracy: 0.9846\n",
      "Epoch 168/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0429 - accuracy: 0.9848\n",
      "Epoch 169/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0483 - accuracy: 0.9846\n",
      "Epoch 170/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0406 - accuracy: 0.9881\n",
      "Epoch 171/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0381 - accuracy: 0.9872\n",
      "Epoch 172/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0411 - accuracy: 0.9872\n",
      "Epoch 173/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0427 - accuracy: 0.9872\n",
      "Epoch 174/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0444 - accuracy: 0.9850\n",
      "Epoch 175/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0429 - accuracy: 0.9875\n",
      "Epoch 176/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0415 - accuracy: 0.9861\n",
      "Epoch 177/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0382 - accuracy: 0.9881\n",
      "Epoch 178/200\n",
      "23/23 [==============================] - 0s 607us/step - loss: 0.0367 - accuracy: 0.9872\n",
      "Epoch 179/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0437 - accuracy: 0.9844\n",
      "Epoch 180/200\n",
      "23/23 [==============================] - 0s 563us/step - loss: 0.0476 - accuracy: 0.9833\n",
      "Epoch 181/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0456 - accuracy: 0.9857\n",
      "Epoch 182/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0474 - accuracy: 0.9861\n",
      "Epoch 183/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0433 - accuracy: 0.9864\n",
      "Epoch 184/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0395 - accuracy: 0.9861\n",
      "Epoch 185/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0498 - accuracy: 0.9844\n",
      "Epoch 186/200\n",
      "23/23 [==============================] - 0s 521us/step - loss: 0.0447 - accuracy: 0.9855\n",
      "Epoch 187/200\n",
      "23/23 [==============================] - 0s 645us/step - loss: 0.0396 - accuracy: 0.9881\n",
      "Epoch 188/200\n",
      "23/23 [==============================] - 0s 781us/step - loss: 0.0449 - accuracy: 0.9850\n",
      "Epoch 189/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0410 - accuracy: 0.9866\n",
      "Epoch 190/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0394 - accuracy: 0.9866\n",
      "Epoch 191/200\n",
      "23/23 [==============================] - 0s 521us/step - loss: 0.0385 - accuracy: 0.9892\n",
      "Epoch 192/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0346 - accuracy: 0.9897\n",
      "Epoch 193/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0487 - accuracy: 0.9835\n",
      "Epoch 194/200\n",
      "23/23 [==============================] - 0s 563us/step - loss: 0.0442 - accuracy: 0.9861\n",
      "Epoch 195/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0398 - accuracy: 0.9864\n",
      "Epoch 196/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0373 - accuracy: 0.9875\n",
      "Epoch 197/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0469 - accuracy: 0.9837\n",
      "Epoch 198/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0492 - accuracy: 0.9833\n",
      "Epoch 199/200\n",
      "23/23 [==============================] - 0s 564us/step - loss: 0.0574 - accuracy: 0.9798\n",
      "Epoch 200/200\n",
      "23/23 [==============================] - 0s 520us/step - loss: 0.0406 - accuracy: 0.9875\n",
      "143/143 [==============================] - 0s 318us/step - loss: 0.0350 - accuracy: 0.9892\n",
      "accuracy : 98.9224%\n",
      " 1/61 [..............................] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "61/61 [==============================] - 0s 343us/step - loss: 0.0464 - accuracy: 0.9856\n",
      "accuracy : 98.5641%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers # lr 조정할때 쓰임\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################\n",
    "# sample로 랜덤 불러오기\n",
    "df_pre = pd.read_csv('wine.csv', header=None)\n",
    "df = df_pre.sample(frac=1) # frac =1 은 100% 불러오기, 0.5는 50%, \n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "dataset = df.values \n",
    "\n",
    "x = dataset[:,:12].astype(float)\n",
    "y = dataset[:,12].astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(\"x_train.shape =\", x_train.shape)\n",
    "print(\"x_test.shape =\", x_test.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "print(y_test[0:5])\n",
    "\n",
    "# 모델 학습과정 설정 \n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train, epochs=200, batch_size=200)\n",
    "score = model.evaluate(x_train,y_train)\n",
    "print(\"%s : %.4f%%\" %(model.metrics_names[1], score[1]*100))\n",
    "\n",
    "score1 = model.evaluate(x_test,y_test)\n",
    "print(\"%s : %.4f%%\" %(model.metrics_names[1], score1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e602e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23471, saving model to .model\\01-0.2347.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.23471 to 0.23226, saving model to .model\\02-0.2323.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23226 to 0.22418, saving model to .model\\03-0.2242.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22418 to 0.20765, saving model to .model\\04-0.2076.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20765 to 0.19059, saving model to .model\\05-0.1906.hdf5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.19059\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19059 to 0.15151, saving model to .model\\07-0.1515.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15151\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15151\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15151 to 0.13390, saving model to .model\\10-0.1339.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13390\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.13390 to 0.13357, saving model to .model\\12-0.1336.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13357\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.13357 to 0.11353, saving model to .model\\14-0.1135.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11353\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11353 to 0.10528, saving model to .model\\16-0.1053.hdf5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10528\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10528\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10528\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10528\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10528 to 0.10051, saving model to .model\\21-0.1005.hdf5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10051\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10051\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.10051 to 0.09833, saving model to .model\\24-0.0983.hdf5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09833\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09833\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09833\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09833\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09833\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09833 to 0.09684, saving model to .model\\30-0.0968.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09684 to 0.09521, saving model to .model\\31-0.0952.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09521 to 0.09337, saving model to .model\\32-0.0934.hdf5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09337\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09337 to 0.09216, saving model to .model\\50-0.0922.hdf5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09216\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09216 to 0.09181, saving model to .model\\58-0.0918.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09181\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09181\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09181\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09181\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09181 to 0.09105, saving model to .model\\63-0.0910.hdf5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09105 to 0.09004, saving model to .model\\64-0.0900.hdf5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.09004\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09004 to 0.08937, saving model to .model\\85-0.0894.hdf5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.08937\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.08937 to 0.08290, saving model to .model\\98-0.0829.hdf5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.08290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00147: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.08290\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.08290 to 0.08116, saving model to .model\\161-0.0812.hdf5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.08116 to 0.08056, saving model to .model\\271-0.0806.hdf5\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.08056 to 0.07876, saving model to .model\\272-0.0788.hdf5\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.07876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00308: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.07876\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.07876 to 0.07606, saving model to .model\\316-0.0761.hdf5\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.07606 to 0.07456, saving model to .model\\317-0.0746.hdf5\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.07456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00470: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.07456\n",
      "\n",
      "Epoch 00578: val_loss improved from 0.07456 to 0.07431, saving model to .model\\578-0.0743.hdf5\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.07431\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.07431\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.07431\n",
      "\n",
      "Epoch 00582: val_loss improved from 0.07431 to 0.07090, saving model to .model\\582-0.0709.hdf5\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00634: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00798: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00962: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01031: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01125: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01194: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01287: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01313: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01314: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01315: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01316: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01317: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01318: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01319: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01320: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01321: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01322: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01323: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01324: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01325: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01326: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01327: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01328: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01329: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01330: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01331: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01332: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01333: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01334: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01335: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01336: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01337: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01338: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01339: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01340: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01341: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01342: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01343: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01344: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01345: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01346: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01347: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01348: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01349: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01350: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01351: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01352: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01353: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01354: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01355: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01356: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01357: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01358: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01359: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01360: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01361: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01362: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01363: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01364: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01365: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01366: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01367: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01368: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01369: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01370: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01371: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01372: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01373: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01374: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01375: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01376: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01377: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01378: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01379: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01380: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01381: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01382: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01383: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01384: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01385: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01386: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01387: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01388: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01389: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01390: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01391: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01392: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01393: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01394: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01395: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01396: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01397: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01398: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01399: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01400: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01401: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01402: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01403: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01404: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01405: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01406: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01407: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01408: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01409: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01410: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01411: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01412: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01413: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01414: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01415: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01416: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01417: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01418: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01419: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01420: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01421: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01422: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01423: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01424: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01425: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01426: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01427: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01428: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01429: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01430: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01431: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01432: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01433: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01434: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01435: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01436: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01437: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01438: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01439: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01440: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01441: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01442: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01443: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01444: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01445: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01446: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01447: val_loss did not improve from 0.07090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01448: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01449: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01450: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01451: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01452: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01453: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01454: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01455: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01456: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01457: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01458: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01459: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01460: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01461: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01462: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01463: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01464: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01465: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01466: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01467: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01468: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01469: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01470: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01471: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01472: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01473: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01474: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01475: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01476: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01477: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01478: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01479: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01480: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01481: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01482: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01483: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01484: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01485: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01486: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01487: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01488: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01489: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01490: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01491: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01492: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01493: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01494: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01495: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01496: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01497: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01498: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01499: val_loss did not improve from 0.07090\n",
      "\n",
      "Epoch 01500: val_loss did not improve from 0.07090\n",
      "143/143 [==============================] - 0s 342us/step - loss: 0.0325 - accuracy: 0.9941\n",
      "accuracy : 99.4062%\n",
      "61/61 [==============================] - 0s 327us/step - loss: 0.0641 - accuracy: 0.9928\n",
      "accuracy : 99.2821%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################\n",
    "# 모델 저장 및 실행\n",
    "#import os\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#######################\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "df_pre = pd.read_csv('wine.csv', header=None)\n",
    "df = df_pre.sample(frac=1) # frac =1 은 100% 불러오기, 0.5는 50%, \n",
    "\n",
    "dataset = df.values \n",
    "x = dataset[:,:12]\n",
    "y = dataset[:,12]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "# 모델 업데이트 하기\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#폴더설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "#모델저장조건 설정\n",
    "modelpath = '.model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, moniter='val_loss', verbose=1,\n",
    "                               save_best_only = True)\n",
    "\n",
    "#모델실행시 validation_split을 위해 데이터의 일부를 자동으로 예약 할 데이터의 비율을 나타내므로 0보다 크고 1보다 작은수로 설정\n",
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=1500, batch_size=200, verbose=0, callbacks=[checkpointer])\n",
    "\n",
    "score = model.evaluate(x_train,y_train)\n",
    "print(\"%s : %.4f%%\" %(model.metrics_names[1], score[1]*100))\n",
    "\n",
    "score1 = model.evaluate(x_test,y_test)\n",
    "print(\"%s : %.4f%%\" %(model.metrics_names[1], score1[1]*100))\n",
    "\n",
    "#테스트셋으로 실험 오차값 저장\n",
    "y_loss = history.history['loss']\n",
    "y_vloss = history.history['val_loss']\n",
    "#학습셋으로 정확도 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "y_vacc = history.history['val_accuracy']\n",
    "#x값을 지저앟고 정확도를 파란색, 오차를 빨간색\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vacc, \"o\", c=\"red\", markersize=3, label='val_accuracy')\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3, label='accuracy')\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"green\", markersize=3, label='val_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"pink\", markersize=3, label='loss')\n",
    "plt.legend(loc='right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24018d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27621, saving model to .model\\01-0.2762.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27621 to 0.18583, saving model to .model\\02-0.1858.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.18583 to 0.18467, saving model to .model\\03-0.1847.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.18467 to 0.16009, saving model to .model\\04-0.1601.hdf5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16009\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16009 to 0.12628, saving model to .model\\06-0.1263.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12628 to 0.10570, saving model to .model\\07-0.1057.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10570 to 0.09717, saving model to .model\\08-0.0972.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09717\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09717 to 0.09212, saving model to .model\\10-0.0921.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09212\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09212\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09212 to 0.08661, saving model to .model\\13-0.0866.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08661 to 0.08414, saving model to .model\\14-0.0841.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08414\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08414 to 0.08242, saving model to .model\\16-0.0824.hdf5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08242\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08242\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08242\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08242\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08242 to 0.08097, saving model to .model\\21-0.0810.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08097 to 0.07730, saving model to .model\\22-0.0773.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.07730 to 0.07562, saving model to .model\\23-0.0756.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07562\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.07562 to 0.07532, saving model to .model\\25-0.0753.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07532 to 0.07435, saving model to .model\\26-0.0743.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.07435 to 0.07234, saving model to .model\\27-0.0723.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07234\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07234 to 0.07092, saving model to .model\\29-0.0709.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.07092\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.07092 to 0.07028, saving model to .model\\52-0.0703.hdf5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.07028\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.07028 to 0.06799, saving model to .model\\88-0.0680.hdf5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.06799\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.06799 to 0.06625, saving model to .model\\146-0.0663.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00147: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.06625\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.06625 to 0.06492, saving model to .model\\174-0.0649.hdf5\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.06492\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.06492\n",
      "  1/143 [..............................] - ETA: 0s - loss: 0.0200 - accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "143/143 [==============================] - 0s 328us/step - loss: 0.0829 - accuracy: 0.9712\n",
      "61/61 [==============================] - 0s 360us/step - loss: 0.0786 - accuracy: 0.9718\n",
      "0.9712 : 0.9718\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################\n",
    "# Early Stopping!!\n",
    "#######################\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import optimizers # lr 조정할때 쓰임\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "df_pre = pd.read_csv('wine.csv', header=None)\n",
    "\n",
    "df = df_pre.sample(frac=1) # frac =1 은 100% 불러오기, 0.5는 50%, 과적합문제때문에 vaildation 0.33을 해야하고 서플도 해야함\n",
    "\n",
    "dataset = df.values \n",
    "x = dataset[:,:12]\n",
    "y = dataset[:,12]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n",
    "\n",
    "# 자동중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "#모델저장조건 설정\n",
    "modelpath = '.model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1,save_best_only = True)\n",
    "\n",
    "#모델실행시 validation_split을 위해 데이터의 일부를 자동으로 예약 할 데이터의 비율을 나타내므로 0보다 크고 1보다 작은수로 설정\n",
    "history = model.fit(x_train, y_train, validation_split=0.33, epochs=3500, batch_size=50, verbose=0,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "print(\"%.4f : %.4f\" %(model.evaluate(x_train,y_train)[1], model.evaluate(x_test,y_test)[1]))\n",
    "\n",
    "#테스트셋으로 실험 오차값 저장\n",
    "y_loss = history.history['loss']\n",
    "y_vloss = history.history['val_loss']\n",
    "#학습셋으로 정확도 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "y_vacc = history.history['val_accuracy']\n",
    "#x값을 지저앟고 정확도를 파란색, 오차를 빨간색\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vacc, \"o\", c=\"red\", markersize=3, label='val_accuracy')\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3, label='accuracy')\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"green\", markersize=3, label='val_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"pink\", markersize=3, label='loss')\n",
    "plt.legend(loc='right')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762cd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
