{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6c201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "input_data = [1,2,3,4,5]\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = x * 2\n",
    "sess = tf.Session()\n",
    "result = sess.run(y, feed_dict={x:input_data})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc2d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:300, a1 = 0.7668, a2 = -0.4862, b = -2.4275, loss = 0.2669\n",
      "step:600, a1 = 0.7891, a2 = -0.2440, b = -3.9078, loss = 0.1906\n",
      "step:900, a1 = 0.7099, a2 = 0.0686, b = -4.9724, loss = 0.1490\n",
      "step:1200, a1 = 0.6129, a2 = 0.3656, b = -5.8134, loss = 0.1221\n",
      "step:1500, a1 = 0.5185, a2 = 0.6323, b = -6.5106, loss = 0.1032\n",
      "step:1800, a1 = 0.4322, a2 = 0.8687, b = -7.1066, loss = 0.0892\n",
      "step:2100, a1 = 0.3551, a2 = 1.0780, b = -7.6275, loss = 0.0785\n",
      "step:2400, a1 = 0.2865, a2 = 1.2642, b = -8.0901, loss = 0.0700\n",
      "step:2700, a1 = 0.2255, a2 = 1.4309, b = -8.5063, loss = 0.0632\n",
      "step:3000, a1 = 0.1712, a2 = 1.5811, b = -8.8847, loss = 0.0575\n",
      "predicted= [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "check predicted = [[0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "check hypothesis = [[0.30852098]\n",
      " [0.67522898]\n",
      " [0.42701229]]\n",
      "check predicted = [[1.]]\n",
      "check hypothesis = [[0.85821142]]\n",
      "\n",
      "Hypotesis: [[0.02189856]\n",
      " [0.03055491]\n",
      " [0.17746311]\n",
      " [0.87777327]\n",
      " [0.98006376]\n",
      " [0.99703718]\n",
      " [0.99956608]] \n",
      "Correct (Y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "x_data = np.array([[2,3],[4,3],[6,4],[8,6],[10,7],[12,8],[14,9]])\n",
    "y_data = np.array([0,0,0,1,1,1,1]).reshape(7,1)\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float64, shape=[None, 1])\n",
    "\n",
    "#기울기 a와 바이어스 b값을 임의로 정함, a는 a1 a2로 변환\n",
    "a = tf.Variable(tf.random_uniform([2,1], dtype=tf.float64)) # [2,1]은 들어오는 값 2개 나가는 값 1개\n",
    "b = tf.Variable(tf.random_uniform([1], dtype=tf.float64))\n",
    "\n",
    "y = tf.sigmoid(tf.matmul(X, a) + b) # ax+b의 시그모이드\n",
    "\n",
    "loss = -tf.reduce_mean(Y * tf.log(y) + (1-Y) * tf.log(1-y))\n",
    "                      \n",
    "lr = 0.1\n",
    "\n",
    "gradient_decent = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(y > 0.5, dtype=tf.float64)  #cast함수는 조건이 맞으면 1, 아니면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(3001):\n",
    "        a_,b_,loss_, _ = sess.run([a,b,loss,gradient_decent], feed_dict={X:x_data, Y:y_data})\n",
    "        if (i+1) % 300 == 0:\n",
    "            print(\"step:%d, a1 = %.4f, a2 = %.4f, b = %.4f, loss = %.4f\"%(i+1, a_[0],a_[1],b_,loss_))\n",
    "            \n",
    "    print(\"predicted=\", sess.run(predicted, feed_dict={X:x_data}))\n",
    "    \n",
    "    p_val, h_val = sess.run([predicted, y], feed_dict={X:[[1,5],[10,5],[4,5]]})\n",
    "    print(\"check predicted =\", p_val)\n",
    "    print(\"check hypothesis =\", h_val)\n",
    "    \n",
    "    p_val, h_val = sess.run([predicted, y], feed_dict={X:[[7,6]]})\n",
    "    print(\"check predicted =\", p_val)\n",
    "    print(\"check hypothesis =\", h_val)\n",
    "    \n",
    "    h, c, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bdd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:300, a1 = -0.4649, a2 = -1.0280, a3 = 0.2508, a4 = -0.2104, a5 = -0.1650, a6 = -0.6958, a7 = 0.0068, a8 = -0.2987, b = 0.4100, loss = 0.5492\n",
      "step:600, a1 = -0.6924, a2 = -2.0014, a3 = 0.0815, a4 = -0.4005, a5 = -0.3171, a6 = -1.1526, a7 = -0.3288, a8 = -0.2262, b = 0.2971, loss = 0.4994\n",
      "step:900, a1 = -0.7776, a2 = -2.5327, a3 = 0.0307, a4 = -0.4871, a5 = -0.3484, a6 = -1.4476, a7 = -0.5483, a8 = -0.1591, b = 0.2652, loss = 0.4844\n",
      "step:1200, a1 = -0.8181, a2 = -2.8597, a3 = 0.0241, a4 = -0.5331, a5 = -0.3493, a6 = -1.6592, a7 = -0.6916, a8 = -0.1143, b = 0.2515, loss = 0.4783\n",
      "step:1500, a1 = -0.8411, a2 = -3.0741, a3 = 0.0376, a4 = -0.5592, a5 = -0.3431, a6 = -1.8201, a7 = -0.7865, a8 = -0.0872, b = 0.2422, loss = 0.4755\n",
      "step:1800, a1 = -0.8559, a2 = -3.2198, a3 = 0.0608, a4 = -0.5741, a5 = -0.3365, a6 = -1.9472, a7 = -0.8502, a8 = -0.0722, b = 0.2340, loss = 0.4741\n",
      "step:2100, a1 = -0.8660, a2 = -3.3212, a3 = 0.0886, a4 = -0.5822, a5 = -0.3314, a6 = -2.0501, a7 = -0.8933, a8 = -0.0650, b = 0.2260, loss = 0.4733\n",
      "step:2400, a1 = -0.8733, a2 = -3.3928, a3 = 0.1183, a4 = -0.5859, a5 = -0.3280, a6 = -2.1350, a7 = -0.9227, a8 = -0.0625, b = 0.2182, loss = 0.4728\n",
      "step:2700, a1 = -0.8786, a2 = -3.4440, a3 = 0.1481, a4 = -0.5866, a5 = -0.3261, a6 = -2.2061, a7 = -0.9428, a8 = -0.0631, b = 0.2106, loss = 0.4725\n",
      "step:3000, a1 = -0.8827, a2 = -3.4809, a3 = 0.1771, a4 = -0.5854, a5 = -0.3254, a6 = -2.2664, a7 = -0.9566, a8 = -0.0654, b = 0.2033, loss = 0.4723\n",
      "predicted= [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "Hypotesis: [[0.37203747]\n",
      " [0.95226414]\n",
      " [0.18969134]\n",
      " [0.96017436]\n",
      " [0.09476857]\n",
      " [0.81250029]\n",
      " [0.95274914]\n",
      " [0.58562772]\n",
      " [0.18958348]\n",
      " [0.57177243]\n",
      " [0.73688618]\n",
      " [0.11993233]\n",
      " [0.26632469]\n",
      " [0.2221669 ]\n",
      " [0.7769232 ]\n",
      " [0.3982353 ]\n",
      " [0.77864705]\n",
      " [0.80511623]\n",
      " [0.81859563]\n",
      " [0.56955553]\n",
      " [0.71506954]\n",
      " [0.06408865]\n",
      " [0.70197723]\n",
      " [0.67625401]\n",
      " [0.30438451]\n",
      " [0.9566782 ]\n",
      " [0.5940087 ]\n",
      " [0.6858619 ]\n",
      " [0.71953333]\n",
      " [0.3953251 ]\n",
      " [0.9665549 ]\n",
      " [0.93428796]\n",
      " [0.63313516]\n",
      " [0.86220431]\n",
      " [0.34890309]\n",
      " [0.68723862]\n",
      " [0.84121934]\n",
      " [0.53246241]\n",
      " [0.34326463]\n",
      " [0.32892178]\n",
      " [0.89680225]\n",
      " [0.10516131]\n",
      " [0.37891299]\n",
      " [0.02890026]\n",
      " [0.55193748]\n",
      " [0.95880796]\n",
      " [0.70393489]\n",
      " [0.73041815]\n",
      " [0.96341861]\n",
      " [0.94686085]\n",
      " [0.95405517]\n",
      " [0.19069162]\n",
      " [0.29600071]\n",
      " [0.97966609]\n",
      " [0.12878018]\n",
      " [0.44121964]\n",
      " [0.09523128]\n",
      " [0.67731551]\n",
      " [0.89958203]\n",
      " [0.49626596]\n",
      " [0.97497462]\n",
      " [0.70821598]\n",
      " [0.67835223]\n",
      " [0.89007066]\n",
      " [0.64592226]\n",
      " [0.58451763]\n",
      " [0.97413232]\n",
      " [0.70276821]\n",
      " [0.86835533]\n",
      " [0.6584225 ]\n",
      " [0.22995054]\n",
      " [0.72426743]\n",
      " [0.94456112]\n",
      " [0.94585247]\n",
      " [0.92465516]\n",
      " [0.80866375]\n",
      " [0.33262103]\n",
      " [0.89905782]\n",
      " [0.91735425]\n",
      " [0.94107246]\n",
      " [0.90447818]\n",
      " [0.87117162]\n",
      " [0.31573266]\n",
      " [0.83209577]\n",
      " [0.54223077]\n",
      " [0.87185052]\n",
      " [0.35393267]\n",
      " [0.92563709]\n",
      " [0.96871259]\n",
      " [0.78615341]\n",
      " [0.80136766]\n",
      " [0.71681646]\n",
      " [0.74982253]\n",
      " [0.54152931]\n",
      " [0.92432651]\n",
      " [0.98646738]\n",
      " [0.90749122]\n",
      " [0.53121993]\n",
      " [0.18029756]\n",
      " [0.64980945]\n",
      " [0.72426724]\n",
      " [0.97456942]\n",
      " [0.78607592]\n",
      " [0.76585452]\n",
      " [0.94972707]\n",
      " [0.66084085]\n",
      " [0.93246674]\n",
      " [0.83688749]\n",
      " [0.41464374]\n",
      " [0.28827909]\n",
      " [0.95057602]\n",
      " [0.90228871]\n",
      " [0.35253372]\n",
      " [0.46278128]\n",
      " [0.6424441 ]\n",
      " [0.8598252 ]\n",
      " [0.89582744]\n",
      " [0.95088559]\n",
      " [0.0666937 ]\n",
      " [0.73427279]\n",
      " [0.8676971 ]\n",
      " [0.68354853]\n",
      " [0.65606435]\n",
      " [0.70116977]\n",
      " [0.64583256]\n",
      " [0.83534916]\n",
      " [0.83406173]\n",
      " [0.70065984]\n",
      " [0.4417939 ]\n",
      " [0.38133952]\n",
      " [0.33804727]\n",
      " [0.80318701]\n",
      " [0.9580271 ]\n",
      " [0.82127685]\n",
      " [0.8168609 ]\n",
      " [0.87531677]\n",
      " [0.46984908]\n",
      " [0.80821057]\n",
      " [0.81162177]\n",
      " [0.73963433]\n",
      " [0.8812545 ]\n",
      " [0.6592994 ]\n",
      " [0.50775028]\n",
      " [0.73555127]\n",
      " [0.94561256]\n",
      " [0.76579329]\n",
      " [0.43929139]\n",
      " [0.9551078 ]\n",
      " [0.59837326]\n",
      " [0.84023702]\n",
      " [0.21852832]\n",
      " [0.32220719]\n",
      " [0.05545383]\n",
      " [0.15872555]\n",
      " [0.93310108]\n",
      " [0.89649437]\n",
      " [0.95959743]\n",
      " [0.0650289 ]\n",
      " [0.54044199]\n",
      " [0.78356121]\n",
      " [0.56400816]\n",
      " [0.89685293]\n",
      " [0.44183341]\n",
      " [0.83480624]\n",
      " [0.58111228]\n",
      " [0.67235233]\n",
      " [0.752657  ]\n",
      " [0.89817512]\n",
      " [0.82175593]\n",
      " [0.57966957]\n",
      " [0.91596322]\n",
      " [0.87312995]\n",
      " [0.96598143]\n",
      " [0.1601735 ]\n",
      " [0.87060145]\n",
      " [0.13331625]\n",
      " [0.30887482]\n",
      " [0.36718252]\n",
      " [0.93387885]\n",
      " [0.63526628]\n",
      " [0.94520376]\n",
      " [0.94271724]\n",
      " [0.62328038]\n",
      " [0.08335188]\n",
      " [0.14930441]\n",
      " [0.63622299]\n",
      " [0.77750074]\n",
      " [0.62895165]\n",
      " [0.88279477]\n",
      " [0.60890636]\n",
      " [0.31972979]\n",
      " [0.11995575]\n",
      " [0.93345404]\n",
      " [0.31039905]\n",
      " [0.90666116]\n",
      " [0.92287866]\n",
      " [0.71348002]\n",
      " [0.60887951]\n",
      " [0.65948541]\n",
      " [0.53281361]\n",
      " [0.75329142]\n",
      " [0.96703426]\n",
      " [0.77297524]\n",
      " [0.86328017]\n",
      " [0.07736697]\n",
      " [0.29308893]\n",
      " [0.92025067]\n",
      " [0.15254082]\n",
      " [0.95619481]\n",
      " [0.21517653]\n",
      " [0.2172349 ]\n",
      " [0.35825399]\n",
      " [0.72503147]\n",
      " [0.13927881]\n",
      " [0.74443266]\n",
      " [0.72658951]\n",
      " [0.85891283]\n",
      " [0.66503298]\n",
      " [0.0964623 ]\n",
      " [0.34925192]\n",
      " [0.75420072]\n",
      " [0.50046788]\n",
      " [0.94646954]\n",
      " [0.95100767]\n",
      " [0.72358104]\n",
      " [0.26849756]\n",
      " [0.02006458]\n",
      " [0.59124931]\n",
      " [0.29050462]\n",
      " [0.37330444]\n",
      " [0.97066316]\n",
      " [0.63466725]\n",
      " [0.9655804 ]\n",
      " [0.14255022]\n",
      " [0.08170334]\n",
      " [0.2436826 ]\n",
      " [0.84686607]\n",
      " [0.93975408]\n",
      " [0.90100406]\n",
      " [0.66027146]\n",
      " [0.66431964]\n",
      " [0.53198423]\n",
      " [0.11369491]\n",
      " [0.56730855]\n",
      " [0.07305571]\n",
      " [0.5581735 ]\n",
      " [0.90121696]\n",
      " [0.68700557]\n",
      " [0.75560147]\n",
      " [0.96960891]\n",
      " [0.83665196]\n",
      " [0.80467546]\n",
      " [0.77961475]\n",
      " [0.79017484]\n",
      " [0.88555593]\n",
      " [0.30430836]\n",
      " [0.31415103]\n",
      " [0.50335607]\n",
      " [0.85048209]\n",
      " [0.65909488]\n",
      " [0.69565959]\n",
      " [0.83107675]\n",
      " [0.26609146]\n",
      " [0.43111002]\n",
      " [0.659391  ]\n",
      " [0.63057504]\n",
      " [0.38274849]\n",
      " [0.92777468]\n",
      " [0.83009231]\n",
      " [0.95314589]\n",
      " [0.57541249]\n",
      " [0.76734717]\n",
      " [0.84981434]\n",
      " [0.84594286]\n",
      " [0.74670417]\n",
      " [0.88931214]\n",
      " [0.29037378]\n",
      " [0.55079214]\n",
      " [0.68873294]\n",
      " [0.35142647]\n",
      " [0.86821155]\n",
      " [0.24359518]\n",
      " [0.56145219]\n",
      " [0.95726512]\n",
      " [0.7889149 ]\n",
      " [0.88189059]\n",
      " [0.6671834 ]\n",
      " [0.41803682]\n",
      " [0.57091769]\n",
      " [0.39870922]\n",
      " [0.37505265]\n",
      " [0.65250832]\n",
      " [0.64392258]\n",
      " [0.65164181]\n",
      " [0.69866674]\n",
      " [0.15538284]\n",
      " [0.64415946]\n",
      " [0.92984735]\n",
      " [0.43155946]\n",
      " [0.68588347]\n",
      " [0.74541955]\n",
      " [0.44031729]\n",
      " [0.73841187]\n",
      " [0.48024307]\n",
      " [0.70238835]\n",
      " [0.93543221]\n",
      " [0.63542248]\n",
      " [0.68971028]\n",
      " [0.86254446]\n",
      " [0.56440207]\n",
      " [0.86056579]\n",
      " [0.96692868]\n",
      " [0.25322266]\n",
      " [0.77367208]\n",
      " [0.22841337]\n",
      " [0.78056879]\n",
      " [0.83934785]\n",
      " [0.72755232]\n",
      " [0.349414  ]\n",
      " [0.80888993]\n",
      " [0.7410629 ]\n",
      " [0.73344059]\n",
      " [0.12732258]\n",
      " [0.80636129]\n",
      " [0.87143323]\n",
      " [0.62997316]\n",
      " [0.95199813]\n",
      " [0.15695403]\n",
      " [0.77964076]\n",
      " [0.96487553]\n",
      " [0.12513017]\n",
      " [0.46217826]\n",
      " [0.72755669]\n",
      " [0.27435749]\n",
      " [0.12156667]\n",
      " [0.86203968]\n",
      " [0.94287609]\n",
      " [0.88539033]\n",
      " [0.64502642]\n",
      " [0.69764724]\n",
      " [0.55322513]\n",
      " [0.74189901]\n",
      " [0.85873683]\n",
      " [0.9554163 ]\n",
      " [0.74910333]\n",
      " [0.78220252]\n",
      " [0.60934287]\n",
      " [0.96128257]\n",
      " [0.95637848]\n",
      " [0.75058519]\n",
      " [0.25523609]\n",
      " [0.66124594]\n",
      " [0.28421131]\n",
      " [0.7817701 ]\n",
      " [0.1298106 ]\n",
      " [0.17735764]\n",
      " [0.4072633 ]\n",
      " [0.74793742]\n",
      " [0.33613382]\n",
      " [0.52416762]\n",
      " [0.84655306]\n",
      " [0.69156225]\n",
      " [0.90080881]\n",
      " [0.96793657]\n",
      " [0.78484962]\n",
      " [0.06026168]\n",
      " [0.43569189]\n",
      " [0.85535518]\n",
      " [0.86349629]\n",
      " [0.63889409]\n",
      " [0.2365397 ]\n",
      " [0.91598572]\n",
      " [0.90493181]\n",
      " [0.19868966]\n",
      " [0.62102223]\n",
      " [0.86282457]\n",
      " [0.89684743]\n",
      " [0.89286581]\n",
      " [0.92969282]\n",
      " [0.89843369]\n",
      " [0.93523121]\n",
      " [0.70479434]\n",
      " [0.62603976]\n",
      " [0.54873127]\n",
      " [0.85578876]\n",
      " [0.89827534]\n",
      " [0.14905057]\n",
      " [0.84327799]\n",
      " [0.90665336]\n",
      " [0.29426911]\n",
      " [0.6196535 ]\n",
      " [0.89805869]\n",
      " [0.52522064]\n",
      " [0.9531687 ]\n",
      " [0.1880461 ]\n",
      " [0.86691308]\n",
      " [0.60789896]\n",
      " [0.91715561]\n",
      " [0.30044012]\n",
      " [0.6215745 ]\n",
      " [0.76826676]\n",
      " [0.85111614]\n",
      " [0.07394116]\n",
      " [0.14818698]\n",
      " [0.72282925]\n",
      " [0.83000841]\n",
      " [0.38558668]\n",
      " [0.80368805]\n",
      " [0.43414563]\n",
      " [0.30013575]\n",
      " [0.8854458 ]\n",
      " [0.41232168]\n",
      " [0.96006369]\n",
      " [0.83273285]\n",
      " [0.63190224]\n",
      " [0.94281697]\n",
      " [0.63880985]\n",
      " [0.82232363]\n",
      " [0.24481351]\n",
      " [0.2002804 ]\n",
      " [0.77965612]\n",
      " [0.33196518]\n",
      " [0.42043875]\n",
      " [0.91884399]\n",
      " [0.93436737]\n",
      " [0.93383328]\n",
      " [0.96542911]\n",
      " [0.73225125]\n",
      " [0.92827035]\n",
      " [0.27977064]\n",
      " [0.3253128 ]\n",
      " [0.47706867]\n",
      " [0.96896306]\n",
      " [0.62936152]\n",
      " [0.13272488]\n",
      " [0.94475672]\n",
      " [0.81475941]\n",
      " [0.61526689]\n",
      " [0.82062211]\n",
      " [0.0063371 ]\n",
      " [0.9427812 ]\n",
      " [0.79080263]\n",
      " [0.75807577]\n",
      " [0.77231404]\n",
      " [0.97944335]\n",
      " [0.65706904]\n",
      " [0.77357826]\n",
      " [0.80743198]\n",
      " [0.84872317]\n",
      " [0.12970485]\n",
      " [0.6497068 ]\n",
      " [0.93038557]\n",
      " [0.63471115]\n",
      " [0.79626733]\n",
      " [0.97099906]\n",
      " [0.87279467]\n",
      " [0.92352201]\n",
      " [0.61786278]\n",
      " [0.81043204]\n",
      " [0.95632849]\n",
      " [0.74553349]\n",
      " [0.66308854]\n",
      " [0.21118267]\n",
      " [0.41331416]\n",
      " [0.50547153]\n",
      " [0.57909004]\n",
      " [0.5656072 ]\n",
      " [0.81191616]\n",
      " [0.61318237]\n",
      " [0.81001782]\n",
      " [0.86558339]\n",
      " [0.76863041]\n",
      " [0.70095432]\n",
      " [0.4255904 ]\n",
      " [0.59949358]\n",
      " [0.95216977]\n",
      " [0.86194377]\n",
      " [0.17383171]\n",
      " [0.35282649]\n",
      " [0.44009956]\n",
      " [0.05695466]\n",
      " [0.92214386]\n",
      " [0.12378974]\n",
      " [0.91228579]\n",
      " [0.91414467]\n",
      " [0.86250725]\n",
      " [0.69459694]\n",
      " [0.91668515]\n",
      " [0.35046576]\n",
      " [0.82598711]\n",
      " [0.956714  ]\n",
      " [0.25019995]\n",
      " [0.40760233]\n",
      " [0.90974633]\n",
      " [0.89673367]\n",
      " [0.66012215]\n",
      " [0.8278851 ]\n",
      " [0.84118111]\n",
      " [0.8601388 ]\n",
      " [0.19857009]\n",
      " [0.77023546]\n",
      " [0.91768418]\n",
      " [0.69485986]\n",
      " [0.84311572]\n",
      " [0.73052437]\n",
      " [0.87462506]\n",
      " [0.90512804]\n",
      " [0.94695241]\n",
      " [0.54475198]\n",
      " [0.40744606]\n",
      " [0.81773114]\n",
      " [0.82635916]\n",
      " [0.98183591]\n",
      " [0.77598011]\n",
      " [0.70601839]\n",
      " [0.39350008]\n",
      " [0.72337289]\n",
      " [0.95403578]\n",
      " [0.97109593]\n",
      " [0.91444887]\n",
      " [0.70511721]\n",
      " [0.71951572]\n",
      " [0.81088798]\n",
      " [0.42788383]\n",
      " [0.81912299]\n",
      " [0.84135454]\n",
      " [0.91228353]\n",
      " [0.60162111]\n",
      " [0.77422724]\n",
      " [0.946489  ]\n",
      " [0.46180068]\n",
      " [0.53070732]\n",
      " [0.65710999]\n",
      " [0.72714649]\n",
      " [0.71112514]\n",
      " [0.91968086]\n",
      " [0.94364564]\n",
      " [0.15163809]\n",
      " [0.07403353]\n",
      " [0.74910622]\n",
      " [0.48299603]\n",
      " [0.2204788 ]\n",
      " [0.87593492]\n",
      " [0.92534154]\n",
      " [0.77001733]\n",
      " [0.95090912]\n",
      " [0.92709215]\n",
      " [0.7935452 ]\n",
      " [0.85677768]\n",
      " [0.7560998 ]\n",
      " [0.50908984]\n",
      " [0.82230055]\n",
      " [0.62538336]\n",
      " [0.05897749]\n",
      " [0.91429873]\n",
      " [0.90265559]\n",
      " [0.77123643]\n",
      " [0.93515754]\n",
      " [0.87172975]\n",
      " [0.90292615]\n",
      " [0.548234  ]\n",
      " [0.67652046]\n",
      " [0.92053856]\n",
      " [0.81063424]\n",
      " [0.86757224]\n",
      " [0.91601892]\n",
      " [0.6036891 ]\n",
      " [0.78136098]\n",
      " [0.84898635]\n",
      " [0.52717811]\n",
      " [0.54768419]\n",
      " [0.06641959]\n",
      " [0.20864088]\n",
      " [0.86712598]\n",
      " [0.68621213]\n",
      " [0.6938466 ]\n",
      " [0.59668745]\n",
      " [0.95850448]\n",
      " [0.39974801]\n",
      " [0.8635217 ]\n",
      " [0.22002856]\n",
      " [0.9409892 ]\n",
      " [0.28969867]\n",
      " [0.77612388]\n",
      " [0.58478169]\n",
      " [0.88663113]\n",
      " [0.58167393]\n",
      " [0.16828767]\n",
      " [0.81197122]\n",
      " [0.94588384]\n",
      " [0.31125947]\n",
      " [0.93320107]\n",
      " [0.90987649]\n",
      " [0.89716741]\n",
      " [0.84492796]\n",
      " [0.36577619]\n",
      " [0.26447527]\n",
      " [0.68514114]\n",
      " [0.1180611 ]\n",
      " [0.96963189]\n",
      " [0.26560363]\n",
      " [0.94340761]\n",
      " [0.8896652 ]\n",
      " [0.3259066 ]\n",
      " [0.15387306]\n",
      " [0.7327279 ]\n",
      " [0.38279047]\n",
      " [0.88144653]\n",
      " [0.77170005]\n",
      " [0.989109  ]\n",
      " [0.60187999]\n",
      " [0.63711277]\n",
      " [0.79355933]\n",
      " [0.86391173]\n",
      " [0.04687002]\n",
      " [0.7289976 ]\n",
      " [0.84113795]\n",
      " [0.86842046]\n",
      " [0.68600299]\n",
      " [0.47319577]\n",
      " [0.61532785]\n",
      " [0.9368619 ]\n",
      " [0.67599719]\n",
      " [0.794843  ]\n",
      " [0.85456339]\n",
      " [0.88219922]\n",
      " [0.85286141]\n",
      " [0.60781503]\n",
      " [0.84156592]\n",
      " [0.92015076]\n",
      " [0.69128559]\n",
      " [0.97426645]\n",
      " [0.83333989]\n",
      " [0.61792073]\n",
      " [0.50176716]\n",
      " [0.87474472]\n",
      " [0.88093808]\n",
      " [0.40716185]\n",
      " [0.6541957 ]\n",
      " [0.13913687]\n",
      " [0.57754053]\n",
      " [0.84337684]\n",
      " [0.9627362 ]\n",
      " [0.83192704]\n",
      " [0.72601349]\n",
      " [0.78546358]\n",
      " [0.89847289]\n",
      " [0.40603677]\n",
      " [0.95237221]\n",
      " [0.55992857]\n",
      " [0.8788193 ]\n",
      " [0.30749561]\n",
      " [0.0445607 ]\n",
      " [0.23920379]\n",
      " [0.29144302]\n",
      " [0.70598969]\n",
      " [0.84483047]\n",
      " [0.59150326]\n",
      " [0.78165468]\n",
      " [0.82305159]\n",
      " [0.46297804]\n",
      " [0.30399163]\n",
      " [0.92948938]\n",
      " [0.92729766]\n",
      " [0.29692613]\n",
      " [0.7268829 ]\n",
      " [0.13273853]\n",
      " [0.41741465]\n",
      " [0.76346945]\n",
      " [0.69081409]\n",
      " [0.92652434]\n",
      " [0.98756486]\n",
      " [0.11038197]\n",
      " [0.68488724]\n",
      " [0.63482416]\n",
      " [0.45241493]\n",
      " [0.72061279]\n",
      " [0.7771633 ]\n",
      " [0.91081662]\n",
      " [0.75236705]\n",
      " [0.4071765 ]\n",
      " [0.73318463]\n",
      " [0.12762582]\n",
      " [0.64432573]\n",
      " [0.48182025]\n",
      " [0.9430939 ]\n",
      " [0.58909359]\n",
      " [0.51556946]\n",
      " [0.84228938]\n",
      " [0.72436386]\n",
      " [0.41244514]\n",
      " [0.74910925]\n",
      " [0.69084984]\n",
      " [0.28647161]\n",
      " [0.57582224]\n",
      " [0.90622567]\n",
      " [0.86572363]\n",
      " [0.58860832]\n",
      " [0.76128839]\n",
      " [0.25113717]\n",
      " [0.85243774]\n",
      " [0.54807096]\n",
      " [0.77138663]\n",
      " [0.3429103 ]\n",
      " [0.65787618]\n",
      " [0.87667701]\n",
      " [0.10376091]\n",
      " [0.23403601]\n",
      " [0.86689708]\n",
      " [0.82056756]\n",
      " [0.82270296]\n",
      " [0.94085407]\n",
      " [0.78540421]\n",
      " [0.68629155]\n",
      " [0.72359337]\n",
      " [0.82394714]\n",
      " [0.70176739]\n",
      " [0.79950735]\n",
      " [0.4759481 ]\n",
      " [0.46310679]\n",
      " [0.91019766]\n",
      " [0.81401083]\n",
      " [0.69648694]\n",
      " [0.18892203]\n",
      " [0.89509705]\n",
      " [0.87842034]\n",
      " [0.85139953]\n",
      " [0.70330981]\n",
      " [0.92144227]\n",
      " [0.8725571 ]\n",
      " [0.79102357]\n",
      " [0.36215632]\n",
      " [0.89783831]\n",
      " [0.92383324]\n",
      " [0.33661908]\n",
      " [0.11099737]\n",
      " [0.76876523]\n",
      " [0.31711229]\n",
      " [0.79482397]\n",
      " [0.22502142]\n",
      " [0.44554269]\n",
      " [0.42471556]\n",
      " [0.76153081]\n",
      " [0.89903115]\n",
      " [0.08813701]\n",
      " [0.34391686]\n",
      " [0.60265453]\n",
      " [0.515928  ]\n",
      " [0.50868402]\n",
      " [0.80929194]\n",
      " [0.12085282]\n",
      " [0.93515165]\n",
      " [0.11345363]\n",
      " [0.90130295]\n",
      " [0.75799365]\n",
      " [0.72286911]\n",
      " [0.85342567]\n",
      " [0.73675864]\n",
      " [0.92353535]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy :  0.7707509881422925\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "data = np.loadtxt(\"C:/Users/tiger/Deeplearning/data-03-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "\n",
    "x_data = np.array([i[0:8] for i in data])\n",
    "y_data = np.array([i[8] for i in data]).reshape(-1,1)\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float64, shape=[None, 1])\n",
    "\n",
    "a = tf.Variable(tf.random_uniform([8,1], dtype=tf.float64)) \n",
    "b = tf.Variable(tf.random_uniform([1], dtype=tf.float64))\n",
    "\n",
    "y = tf.sigmoid(tf.matmul(X, a) + b) # ax+b의 시그모이드\n",
    "\n",
    "loss = -tf.reduce_mean(Y * tf.log(y) + (1-Y) * tf.log(1-y))\n",
    "                      \n",
    "lr = 0.1\n",
    "\n",
    "gradient_decent = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "predicted = tf.cast(y > 0.5, dtype=tf.float64)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(30001):\n",
    "        a_,b_,loss_, _ = sess.run([a,b,loss,gradient_decent], feed_dict={X:x_data, Y:y_data})\n",
    "        if (i+1) % 300 == 0:\n",
    "            print(\"step:%d, a1 = %.4f, a2 = %.4f, a3 = %.4f, a4 = %.4f, a5 = %.4f, a6 = %.4f, a7 = %.4f, a8 = %.4f, b = %.4f, loss = %.4f\"%(i+1, a_[0],a_[1],a_[2],a_[3],a_[4],a_[5],a_[6],a_[7],b_,loss_))\n",
    "            \n",
    "    print(\"predicted=\", sess.run(predicted, feed_dict={X:x_data}))\n",
    "    \n",
    "    h, c, a = sess.run([y, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c6b285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 0.5426625]\n",
      "[599, 0.49952403]\n",
      "[899, 0.4858821]\n",
      "[1199, 0.4800417]\n",
      "[1499, 0.4770941]\n",
      "[1799, 0.47542793]\n",
      "[2099, 0.47439936]\n",
      "[2399, 0.47371837]\n",
      "[2699, 0.47324216]\n",
      "[2999, 0.47289523]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "data = np.loadtxt(\"C:/Users/tiger/Deeplearning/data-03-diabetes.csv\", delimiter=\",\", dtype =np.float32)\n",
    "\n",
    "x_data = data[:, 0:-1]\n",
    "y_data = data[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([8,1]), name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b) # ax+b의 시그모이드\n",
    "                \n",
    "lr = 0.1\n",
    "                \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "                      \n",
    "predicted = tf.cast(y > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(3001):\n",
    "        cost_val, _ = sess.run([cost,train], feed_dict={X:x_data, Y:y_data})\n",
    "        if (i+1) % 300 == 0:\n",
    "            print([i,cost_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1656789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 0.2186459 [[2.1134315]\n",
      " [2.1341856]] [-3.4460254]\n",
      "999 0.13958043 [[3.094363 ]\n",
      " [3.0977824]] [-4.8595495]\n",
      "1499 0.10220708 [[3.7619016]\n",
      " [3.7627544]] [-5.843028]\n",
      "1999 0.08032183 [[4.271352]\n",
      " [4.271621]] [-6.5987406]\n",
      "2499 0.06598582 [[4.683229]\n",
      " [4.683334]] [-7.211626]\n",
      "2999 0.05589556 [[5.0285797]\n",
      " [5.028629 ]] [-7.7264166]\n",
      "3499 0.048424672 [[5.32564 ]\n",
      " [5.325663]] [-8.16972]\n",
      "3999 0.042679682 [[5.586069]\n",
      " [5.586082]] [-8.558666]\n",
      "4499 0.038130097 [[5.817783 ]\n",
      " [5.8177905]] [-8.9049225]\n",
      "4999 0.03444134 [[6.026394 ]\n",
      " [6.0263996]] [-9.216797]\n",
      "5499 0.03139242 [[6.2160306]\n",
      " [6.2160344]] [-9.500402]\n",
      "5999 0.028831571 [[6.389809]\n",
      " [6.389812]] [-9.7603655]\n",
      "6499 0.026651282 [[6.550143]\n",
      " [6.550145]] [-10.000274]\n",
      "6999 0.024773207 [[6.6989384]\n",
      " [6.6989403]] [-10.22296]\n",
      "7499 0.023139123 [[6.8377237]\n",
      " [6.837725 ]] [-10.4307]\n",
      "7999 0.021704735 [[6.9677463]\n",
      " [6.967747 ]] [-10.625352]\n",
      "8499 0.020435747 [[7.0900335]\n",
      " [7.090034 ]] [-10.808448]\n",
      "8999 0.019305382 [[7.205446]\n",
      " [7.205446]] [-10.981271]\n",
      "9499 0.018292174 [[7.314708]\n",
      " [7.314708]] [-11.1448965]\n",
      "9999 0.017378962 [[7.418434]\n",
      " [7.418434]] [-11.300249]\n",
      "\n",
      "Hypotesis: [[1.2366092e-05]\n",
      " [2.0195067e-02]\n",
      " [2.0195067e-02]\n",
      " [9.7171474e-01]] \n",
      "Correct (Y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[0],[0],[1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b) # ax+b의 시그모이드\n",
    "                \n",
    "lr = 0.1\n",
    "                \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w), sess.run(b))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5630bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 0.15777668 [[2.7829435]\n",
      " [2.8455837]] [-0.7794236]\n",
      "999 0.09036717 [[3.9926262]\n",
      " [4.0147767]] [-1.4620613]\n",
      "1499 0.06233677 [[4.7769346]\n",
      " [4.7877054]] [-1.8803422]\n",
      "1999 0.04726307 [[5.353131]\n",
      " [5.359382]] [-2.1813858]\n",
      "2499 0.03793314 [[5.8068924]\n",
      " [5.8109446]] [-2.4159608]\n",
      "2999 0.03162002 [[6.180396 ]\n",
      " [6.1832156]] [-2.6077929]\n",
      "3499 0.027077172 [[6.4973674]\n",
      " [6.4994397]] [-2.7698824]\n",
      "3999 0.023657717 [[6.7724557]\n",
      " [6.77404  ]] [-2.9101093]\n",
      "4499 0.020994226 [[7.015305]\n",
      " [7.016553]] [-3.0336075]\n",
      "4999 0.01886266 [[7.2325926]\n",
      " [7.233603 ]] [-3.1439006]\n",
      "5499 0.017119285 [[7.4291334]\n",
      " [7.429965 ]] [-3.2435138]\n",
      "5999 0.015667627 [[7.6085033]\n",
      " [7.6091995]] [-3.3343117]\n",
      "6499 0.014440559 [[7.7734337]\n",
      " [7.7740254]] [-3.4177153]\n",
      "6999 0.013389988 [[7.9260554]\n",
      " [7.926565 ]] [-3.494828]\n",
      "7499 0.012480587 [[8.068064]\n",
      " [8.068507]] [-3.5665243]\n",
      "7999 0.011685858 [[8.200828]\n",
      " [8.201217]] [-3.6335099]\n",
      "8499 0.010985538 [[8.325467]\n",
      " [8.325812]] [-3.6963596]\n",
      "8999 0.010363767 [[8.442909]\n",
      " [8.443218]] [-3.755553]\n",
      "9499 0.009808102 [[8.553939]\n",
      " [8.554218]] [-3.8114884]\n",
      "9999 0.009308632 [[8.659209]\n",
      " [8.65946 ]] [-3.864502]\n",
      "\n",
      "Hypotesis: [[0.02054045]\n",
      " [0.9917973 ]\n",
      " [0.9917953 ]\n",
      " [0.99999857]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[1]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b) # ax+b의 시그모이드\n",
    "                \n",
    "lr = 0.1\n",
    "                \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w), sess.run(b))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca42065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 0.69321036 [[0.03292246]\n",
      " [0.02841607]] [-0.03637835]\n",
      "999 0.6931484 [[0.00439467]\n",
      " [0.0041986 ]] [-0.00509641]\n",
      "1499 0.6931472 [[0.00060619]\n",
      " [0.00059766]] [-0.00071396]\n",
      "1999 0.6931472 [[8.450390e-05]\n",
      " [8.413205e-05]] [-0.00010001]\n",
      "2499 0.6931472 [[1.1808582e-05]\n",
      " [1.1792870e-05]] [-1.3985149e-05]\n",
      "2999 0.69314724 [[1.6474771e-06]\n",
      " [1.6466663e-06]] [-1.9457552e-06]\n",
      "3499 0.6931472 [[2.0652959e-07]\n",
      " [2.0571878e-07]] [-2.939594e-07]\n",
      "3999 0.6931472 [[8.8810118e-08]\n",
      " [8.7999304e-08]] [-5.9265222e-08]\n",
      "4499 0.6931472 [[8.8810118e-08]\n",
      " [8.7999304e-08]] [-5.9265222e-08]\n",
      "4999 0.6931472 [[8.8810118e-08]\n",
      " [8.7999304e-08]] [-5.9265222e-08]\n",
      "\n",
      "Hypotesis: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct (Y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy :  0.5\n",
      "[[8.8810118e-08]\n",
      " [8.7999304e-08]] [-5.9265222e-08]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w = tf.Variable(tf.random_uniform([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_uniform([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b) # ax+b의 시그모이드\n",
    "                \n",
    "lr = 0.1                \n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5000):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w), sess.run(b))\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    print(sess.run(w), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "823fabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\n",
      "입력값 : (0, 0)출력값 : 0\n",
      "입력값 : (1, 0)출력값 : 1\n",
      "입력값 : (0, 1)출력값 : 1\n",
      "입력값 : (1, 1)출력값 : 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w11 = np.array([-2,-2])\n",
    "w12 = np.array([2,2])\n",
    "w2 = np.array([1,1])\n",
    "b1 = 3\n",
    "b2 = -1\n",
    "b3 = -1\n",
    "print(np.sum(w11*x))\n",
    "\n",
    "def MLP(x,w,b):\n",
    "    y = np.sum(w*x) + b # 시그노이드함수의 z값을 보여줌\n",
    "    if y <= 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def NAND(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w11, b1)\n",
    "\n",
    "def OR(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w12, b2)\n",
    "\n",
    "def AND(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w2, b3)\n",
    "\n",
    "def XOR(x1,x2):\n",
    "    return AND(NAND(x1,x2),OR(x1,x2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for x in [(0,0),(1,0),(0,1),(1,1)]:\n",
    "        y = XOR(x[0], x[1])\n",
    "        print(\"입력값 : \" + str(x) + \"출력값 : \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2c3a47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 : (0, 0)출력값 : 0\n",
      "입력값 : (1, 0)출력값 : 1\n",
      "입력값 : (0, 1)출력값 : 1\n",
      "입력값 : (1, 1)출력값 : 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "w11 = np.array([-7.40,-7.40])\n",
    "w12 = np.array([8.67,8.67])\n",
    "w2 = np.array([7.41,7.41])\n",
    "b1 = 11.28\n",
    "b2 = -3.87\n",
    "b3 = -11.29\n",
    "\n",
    "def MLP(x,w,b):\n",
    "    y = 1 / (1 + np.exp(-(np.dot(w,x) + b))) #시그노이드함수\n",
    "    if y <= 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def NAND(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w11, b1)\n",
    "\n",
    "def OR(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w12, b2)\n",
    "\n",
    "def AND(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w2, b3)\n",
    "\n",
    "def XOR(x1,x2):\n",
    "    return AND(NAND(x1,x2),OR(x1,x2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for x in [(0,0),(1,0),(0,1),(1,1)]:\n",
    "        y = XOR(x[0], x[1])\n",
    "        print(\"입력값 : \" + str(x) + \"출력값 : \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21f7dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 0.16336694 [[ 5.209755]\n",
      " [-6.571386]] [-2.0145965]\n",
      "9999 0.0198116 [[  9.42106 ]\n",
      " [-10.025889]] [-4.3100715]\n",
      "14999 0.009919811 [[ 10.709085]\n",
      " [-11.234259]] [-4.983059]\n",
      "19999 0.0065592844 [[ 11.47637 ]\n",
      " [-11.971914]] [-5.379329]\n",
      "\n",
      "Hypotesis: [[0.0054296 ]\n",
      " [0.9941136 ]\n",
      " [0.9941156 ]\n",
      " [0.00894687]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n",
      "[[-1.904905    1.5564839 ]\n",
      " [ 0.14761186  0.06333599]] [-1.3327322]\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#다중 로지스틱스회귀 / 오차역전파 \n",
    "#######################################\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,w1) + b1)\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2) # ax+b의 시그모이드\n",
    "\n",
    "lr = 0.1           \n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w2), sess.run(b2))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    print(sess.run(w), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78d78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 0.036105342 [[ 7.394007  ]\n",
      " [ 1.3070761 ]\n",
      " [ 0.60752285]\n",
      " [-3.7471917 ]\n",
      " [-9.167669  ]] [-0.76491165]\n",
      "9999 0.010486992 [[  9.170415  ]\n",
      " [  2.1625342 ]\n",
      " [  0.61780924]\n",
      " [ -4.596175  ]\n",
      " [-10.722207  ]] [-1.2358935]\n",
      "14999 0.005757644 [[  9.928315 ]\n",
      " [  2.6371124]\n",
      " [  0.6177353]\n",
      " [ -5.0163436]\n",
      " [-11.418203 ]] [-1.4445364]\n",
      "19999 0.0038792547 [[ 10.397446 ]\n",
      " [  2.9667504]\n",
      " [  0.6177432]\n",
      " [ -5.2941275]\n",
      " [-11.858361 ]] [-1.5743865]\n",
      "\n",
      "Hypotesis: [[0.00325716]\n",
      " [0.9977    ]\n",
      " [0.99498785]\n",
      " [0.00491503]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n",
      "[[ 10.397446 ]\n",
      " [  2.9667504]\n",
      " [  0.6177432]\n",
      " [ -5.2941275]\n",
      " [-11.858361 ]] [-1.5743865]\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#다중 로지스틱스회귀 / 오차역전파 \n",
    "#######################################\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,5]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,w1) + b1)\n",
    "w2 = tf.Variable(tf.random_normal([5,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2) # ax+b의 시그모이드\n",
    "\n",
    "lr = 0.1           \n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w2), sess.run(b2))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    print(sess.run(w2), sess.run(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43772a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 0.06739411 [[ 3.6788073]\n",
      " [-2.6464214]\n",
      " [-4.736465 ]\n",
      " [-1.2181518]\n",
      " [ 0.7704828]] [1.1950116]\n",
      "9999 0.0032807742 [[ 5.4755497]\n",
      " [-3.8310184]\n",
      " [-6.511461 ]\n",
      " [-1.5920771]\n",
      " [ 1.4109153]] [1.5158274]\n",
      "14999 0.0015423245 [[ 5.899085 ]\n",
      " [-4.116598 ]\n",
      " [-6.89895  ]\n",
      " [-1.6918042]\n",
      " [ 1.5853361]] [1.6014633]\n",
      "19999 0.000990222 [[ 6.145693 ]\n",
      " [-4.2837057]\n",
      " [-7.119742 ]\n",
      " [-1.7526735]\n",
      " [ 1.6918716]] [1.6530547]\n",
      "\n",
      "Hypotesis: [[7.2604418e-04]\n",
      " [9.9892211e-01]\n",
      " [9.9892735e-01]\n",
      " [1.0823309e-03]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n",
      "[[ 6.145693 ]\n",
      " [-4.2837057]\n",
      " [-7.119742 ]\n",
      " [-1.7526735]\n",
      " [ 1.6918716]] [1.6530547]\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#다중 로지스틱스회귀 / 오차역전파 \n",
    "#######################################\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,5]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,w1) + b1)\n",
    "w2 = tf.Variable(tf.random_normal([5,5]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,w2) + b2)\n",
    "w3 = tf.Variable(tf.random_normal([5,5]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,w3) + b3)\n",
    "w4 = tf.Variable(tf.random_normal([5,1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, w4) + b4) # ax+b의 시그모이드\n",
    "\n",
    "lr = 0.1           \n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(i, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(w4), sess.run(b4))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    print(sess.run(w4), sess.run(b4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f4aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.41291484]\n",
      " [ 0.31013608]\n",
      " [ 0.44765988]\n",
      " [ 0.65920365]\n",
      " [-0.9078376 ]] [0.48467365]\n",
      "1000 [[ 0.29329526]\n",
      " [ 0.30956364]\n",
      " [ 0.35185617]\n",
      " [ 0.59649384]\n",
      " [-1.0086577 ]] [0.24052285]\n",
      "2000 [[ 0.35335568]\n",
      " [ 0.32339123]\n",
      " [ 0.32987016]\n",
      " [ 0.59023875]\n",
      " [-1.0282575 ]] [0.26543158]\n",
      "3000 [[ 0.4996394 ]\n",
      " [ 0.35123372]\n",
      " [ 0.2988756 ]\n",
      " [ 0.60705847]\n",
      " [-1.1361815 ]] [0.30263945]\n",
      "4000 [[ 0.8447444 ]\n",
      " [ 0.41456285]\n",
      " [ 0.2547402 ]\n",
      " [ 0.6746191 ]\n",
      " [-1.4582287 ]] [0.37040448]\n",
      "5000 [[ 1.8521461 ]\n",
      " [ 0.6061804 ]\n",
      " [ 0.20705876]\n",
      " [ 0.8985851 ]\n",
      " [-2.4594612 ]] [0.61697364]\n",
      "6000 [[ 3.343322  ]\n",
      " [ 0.80481344]\n",
      " [-0.1750899 ]\n",
      " [ 0.9748551 ]\n",
      " [-4.128695  ]] [0.54683834]\n",
      "7000 [[ 5.141275  ]\n",
      " [ 0.9169143 ]\n",
      " [-0.62065744]\n",
      " [ 1.0422976 ]\n",
      " [-6.070036  ]] [0.4594129]\n",
      "8000 [[ 5.7816463 ]\n",
      " [ 0.9672412 ]\n",
      " [-0.73948085]\n",
      " [ 1.1135981 ]\n",
      " [-6.7110624 ]] [0.50134206]\n",
      "9000 [[ 6.145713  ]\n",
      " [ 0.9982915 ]\n",
      " [-0.81175995]\n",
      " [ 1.1564033 ]\n",
      " [-7.074468  ]] [0.5244539]\n",
      "\n",
      "Hypotesis: [[0.00627261]\n",
      " [0.9927472 ]\n",
      " [0.9948735 ]\n",
      " [0.00559065]] \n",
      "Correct (Y): [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy :  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################\n",
    "#다중 로지스틱스회귀 / 오차역전파 \n",
    "#######################################\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "epoch_arr = []\n",
    "cost_arr = []\n",
    "accuracy_arr = []\n",
    "step_val = 10000\n",
    "\n",
    "def graph():\n",
    "    import matplotlib as mpl\n",
    "    mpl.rc('axes', unicode_minus = False)\n",
    "    fig, ax0 = plt.subplots()\n",
    "    ax1 = ax0.twinx()\n",
    "    ax0.set_title(\"Epoch : cost / accuracy\")\n",
    "    ax0.plot(cost_arr, 'r-', label=\"cost\")\n",
    "    ax0.set_ylabel(\"cost\")\n",
    "    ax0.axis([0, step_val,0,1])\n",
    "    ax0.grid(True)\n",
    "    ax1.plot(accuracy_arr, 'b', label=\"accuracy\")\n",
    "    ax1.set_ylabel(\"accuracy\")\n",
    "    ax1.grid(False)\n",
    "    ax1.set_xlabel(\"epochs\")\n",
    "    ax1.axis([0, step_val,0,1])\n",
    "    plt.show()\n",
    "    \n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,5]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,w1) + b1)\n",
    "w2 = tf.Variable(tf.random_normal([5,5]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,w2) + b2)\n",
    "w3 = tf.Variable(tf.random_normal([5,5]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,w3) + b3)\n",
    "w4 = tf.Variable(tf.random_normal([5,1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, w4) + b4) # ax+b의 시그모이드\n",
    "\n",
    "lr = 0.1           \n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(step_val):\n",
    "        _, h, p, a, c = sess.run([train, hypothesis, predicted, accuracy, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        epoch_arr.append(i)\n",
    "        cost_arr.append(c)\n",
    "        accuracy_arr.append(a)\n",
    "\n",
    "        if i % (step_val/10) == 0:\n",
    "            print(i, sess.run(w4), sess.run(b4))\n",
    "            \n",
    "        h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    \n",
    "graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e1b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#다중 로지스틱스회귀 / 오차역전파 \n",
    "#######################################\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "epoch_arr = []\n",
    "cost_arr = []\n",
    "accuracy_arr = []\n",
    "step_val = 10000\n",
    "\n",
    "def graph():\n",
    "    import matplotlib as mpl\n",
    "    mpl.rc('axes', unicode_minus = False)\n",
    "    fig, ax0 = plt.subplots()\n",
    "    ax1 = ax0.twinx()\n",
    "    ax0.set_title(\"Epoch : cost / accuracy\")\n",
    "    ax0.plot(cost_arr, 'r-', label=\"cost\")\n",
    "    ax0.set_ylabel(\"cost\")\n",
    "    ax0.axis([0, step_val,0,1])\n",
    "    ax0.grid(True)\n",
    "    ax1.plot(accuracy_arr, 'b', label=\"accuracy\")\n",
    "    ax1.set_ylabel(\"accuracy\")\n",
    "    ax1.grid(False)\n",
    "    ax1.set_xlabel(\"epochs\")\n",
    "    ax1.axis([0, step_val,0,1])\n",
    "    plt.show()\n",
    "    \n",
    "#실행할때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#data = [[0,0,0,1,1,0,1,1]]\n",
    "#x_data = np.array(data, dtype=np.float32).reshape(-1,2)\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4aac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,5]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([5]), name='bias1')\n",
    "layer1 = tf.nn.relu(tf.matmul(X,w1) + b1)\n",
    "w2 = tf.Variable(tf.random_normal([5,5]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([5]), name='bias2')\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,w2) + b2)\n",
    "w3 = tf.Variable(tf.random_normal([5,5]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([5]), name='bias3')\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2,w3) + b3)\n",
    "w4 = tf.Variable(tf.random_normal([5,5]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([5]), name='bias4')\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3,w4) + b4)\n",
    "w5 = tf.Variable(tf.random_normal([5,5]), name='weight5')\n",
    "b5 = tf.Variable(tf.random_normal([1]), name='bias5')\n",
    "layer5 = tf.nn.relu(tf.matmul(layer4,w5) + b5)\n",
    "w6 = tf.Variable(tf.random_normal([5,1]), name='weight6')\n",
    "b6 = tf.Variable(tf.random_normal([1]), name='bias6')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer5, w6) + b6) # ax+b의 시그모이드\n",
    "\n",
    "lr = 0.01           \n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "                      \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(step_val):\n",
    "        _, h, p, a, c = sess.run([train, hypothesis, predicted, accuracy, cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        epoch_arr.append(i)\n",
    "        cost_arr.append(c)\n",
    "        accuracy_arr.append(a)\n",
    "\n",
    "        if i % (step_val/10) == 0:\n",
    "            print(i, sess.run(w4), sess.run(b4))\n",
    "            \n",
    "        h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypotesis:\", h, \"\\nCorrect (Y):\", c, \"\\nAccuracy : \", a) \n",
    "    \n",
    "graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
